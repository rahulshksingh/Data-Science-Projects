{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 10999  | total loss: \u001b[1m\u001b[32m0.45100\u001b[0m\u001b[0m | time: 0.032s\n",
      "| Adam | epoch: 1000 | loss: 0.45100 - acc: 0.9745 -- iter: 80/86\n",
      "Training Step: 11000  | total loss: \u001b[1m\u001b[32m0.40721\u001b[0m\u001b[0m | time: 0.036s\n",
      "| Adam | epoch: 1000 | loss: 0.40721 - acc: 0.9771 -- iter: 86/86\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Shraddha Singh\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "\n",
    "import json\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "print(data)\n",
    "    \n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "    \n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])]) \n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") # it allow us to get the probability of each output\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shraddha Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Shraddha Singh\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "You: Hello\n",
      "Hello!\n",
      "You: hey\n",
      "Hello!\n",
      "You: Hii\n",
      "Hi there, how can I help?\n",
      "You: what about akeria\n",
      "Powerful and complete.Akeria is young and fast-growing AI based organization providing comprehensive, integrated and robust software solutions, web solutions, Data Science Technology & big data analytics services as well as digital marketing. Our clients and partners include academic, pharmaceutical, biotechnology, hospitality industries, e-commerce, and government organizations.\n",
      "You: meaning of akeria\n",
      "Powerful and complete.Akeria is young and fast-growing AI based organization providing comprehensive, integrated and robust software solutions, web solutions, Data Science Technology & big data analytics services as well as digital marketing. Our clients and partners include academic, pharmaceutical, biotechnology, hospitality industries, e-commerce, and government organizations.\n",
      "You: what are services\n",
      "Akeria specializes in providing Artificial Intelligence Services, Software & ERP development, Web development and design services, mobile app development services, Data Analytics services for all sectors, SEO & Digital marketing services.\n",
      "You: services\n",
      "Akeria specializes in providing Artificial Intelligence Services, Software & ERP development, Web development and design services, mobile app development services, Data Analytics services for all sectors, SEO & Digital marketing services.\n",
      "You: AI\n",
      "Artificial intelligence was founded as an academic discipline in 1956, and in the years   since has experienced several waves of optimism, followed by disappointment and the loss of funding, followed by new approaches, success and renewed funding.Artificial intelligence (AI) is an area of computer science that emphasizes the creation of intelligent machines that work and react like humans. Some of the activities computers with artificial intelligence are designed for include: Speech recognition, Learning, Planning, Problem solving\n",
      "You: what is AI\n",
      "Artificial intelligence was founded as an academic discipline in 1956, and in the years   since has experienced several waves of optimism, followed by disappointment and the loss of funding, followed by new approaches, success and renewed funding.Artificial intelligence (AI) is an area of computer science that emphasizes the creation of intelligent machines that work and react like humans. Some of the activities computers with artificial intelligence are designed for include: Speech recognition, Learning, Planning, Problem solving\n",
      "You: Data analytics\n",
      "Data analytics refers to qualitative and quantitative techniques and processes used to enhance productivity and business gain.Data is extracted and categorized to identify and analyze behavioral data and patterns, and techniques vary according to organizational requirements. Data analytics is also known as data analysis.\n",
      "You: SEo\n",
      "SEO is an acronym that stands for search engine optimization, which is the process of optimizing your website to get organic, or unpaid, traffic from the search engine results page.In other words, SEO involves making certain changes to your website design and content that make your site more attractive to a search engine.\n",
      "You: SMO\n",
      "SMO is an acronym that stands for search media optimization, the process of increasing the awareness of a product, brand or event by using a number of social media outlets and communities to generate viral publicity.\n",
      "You: Digital marketing\n",
      "Digital marketing is the act of promoting and selling products and services by leveraging online marketing tactics such as social media marketing, search marketing, and email marketing.\n",
      "You: SMM\n",
      "Social media marketing (SMM) is a form of Internet marketing that utilizes social networking websites as a marketing tool. The goal of SMM is to produce content that users will share with their social network to help a company increase brand exposure and broaden customer reach.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
